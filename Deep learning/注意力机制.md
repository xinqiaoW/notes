---
share_link: https://share.note.sx/ycx8pfag#mcL7Mxrpy3kDwOd+/JIUe0mhR/+w6MBWWOByikmGhVw
share_updated: 2024-12-03T13:03:17+08:00
---
**主要内容** 注意力池化层会根据 query 有倾向地选择输入中的某些 key-value 对。也就是，会选择与query更相关的信息，从而使得模型的效果更好。

注意力机制既有不带参数的，也有带可学习的参数的。
如Addictive Attention：带有可学习的参数，将 query 和 key 映射到同样的长度，并相加，再用 tanh 激活，再做内积。通过这一系列的操作，将 query 和 key 映射到一个标量上，衡量 key 对 query 的重要性。


- 自注意力：
	自注意力将序列 $x_1, x_2, …… ,x_n$ 同时作为 query，key，和 value。最后的输出为$$y_i=f(x_i, (x_1,x_1), (x_2, x_2), ……,(x_n, x_n))$$  也就是找到对 $x_i$ 更重要的 $x_k$ 赋予更高权重，使模型注意到 $x_k$ 
	由于自注意力没有处理序列的时序信息，所以我们用位置编码矩阵加到输入中，使得模型可以处理时序信息。
	我们的位置编码矩阵还可以反应相对位置信息，我们的模型可能可以通过学习，对编码矩阵 $i+\delta$ 和 $i$ 行（相差 $\delta$ 的两行）所具备的数量特征做出反应。



- 自注意力 和 CNN、RNN 处理序列的不同：
	自注意力在处理序列的时候，可以很容易的将较长的序列中前端信息传到后端，但是针对较长序列来说，自注意力的运算量很大。
	同时，自注意力的并行度很好，计算一个输出时，可以同时计算另外的输出。
	RNN 在以上两点效果都不是太好，但 RNN 处理序列性很强的数据时，由于其架构，效果更好，因为 RNN 每次只会看隐藏信息和现在的输入，不太会被序列中别处的信息干扰。
