---
share_link: https://share.note.sx/1n8uxleq#aGNsx7qJGdDvOwllGfVdT8XJ3ijXuGsSn7bLWbZLYUA
share_updated: 2024-12-03T13:03:00+08:00
---
**主要内容** 我学习了批量归一化，批量归一化固定小批量里面的均值和方差，使得每一批数据通过批量归一化层后，得到的数据的均值方差是一定的（均值方差是我们可学习的参数）。批量归一化层作用于全连接层的特征维，卷积层的通道维，一般用于输出之后，激活函数之前，或者输入之后。批量归一化是对原数据做了一个线性变换，所以放在非线性的激活函数之前，不然显得很奇怪。批量归一化的作用原理可能是通过为数据加入了随机的噪声，因此一般不与丢弃法混用。批量归一化允许我们选择更大的学习率，从而加快模型训练速度（模型收敛更快，不代表模型每秒处理的examples的数目更多）。

- 代码部分：

 1 - 实现批量归一化函数：
```
def batch_norm(X, moving_mean, moving_var, gamma, beta, eps, momentum):  
    if not torch.is_grad_enabled():  
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)  
    else:  
        assert len(X.shape) in (2, 4)  
        if len(X.shape) == 2:  
            mean = X.mean(dim=0)  
            var = ((X - mean) ** 2).mean(dim=0)  
        else:  
            mean = X.mean(dim=(0, 2, 3), keepdim=True)  
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)  
        X_hat = (X - mean) / torch.sqrt(var + eps)  
        moving_mean = momentum* moving_mean + (1.0 - momentum) * mean  
        moving_var = momentum * moving_var + (1.0 - momentum) * var  
    Y = gamma * X_hat + beta  
    return Y, moving_mean.data, moving_var.data
```
2 - 实现批量归一化类：
```
class BatchNorm(nn.Module):  
    def __init__(self):  
        super().__init__()  
        self.eps = 1e-5  
        self.gamma = nn.Parameter(torch.ones(1))  
        self.beta = nn.Parameter(torch.zeros(1))  
        self.moving_mean = torch.zeros(1)  
        self.moving_var = torch.zeros(1)  
        self.momentum = 0.9  
  
    def forward(self, x):  
        if self.moving_mean.device != x.device:  
            self.moving_mean = self.moving_mean.to(x.device)  
            self.moving_var = self.moving_var.to(x.device)  
        Y, self.moving_mean, self.moving_var = batch_norm(x, self.moving_mean, self.moving_var, self.gamma, self.beta, self.eps, self.momentum)  
        return Y
```
3 - 在lenet上添加批量归一化层：
```
net = nn.Sequential(  
    Reshape(),  
    nn.Conv2d(1, 6, kernel_size=5, padding=2),  
    BatchNorm(),  
    nn.Sigmoid(),  
    nn.AvgPool2d(2, stride=2),  
    nn.Conv2d(6, 16, kernel_size=5),  
    BatchNorm(),  
    nn.Sigmoid(),  
    nn.AvgPool2d(2, stride=2),  
    nn.Flatten(),  
    nn.Linear(16 * 5 * 5, 120),  
    BatchNorm(),  
    nn.Sigmoid(),  
    nn.Linear(120, 84),  
    BatchNorm(),  
    nn.Sigmoid(),  
    nn.Linear(84, 10)  
)
```
4 - 有无批量归一化层的对比：
无批量归一化层：
![[le_net_acc 3.gif]]
有批量归一化层：
![[le_net_acc_with_batch_norm.gif]]
可以发现，添加批量归一化层后，模型训练效果提升显著，同样扫了十遍数据，添加了批量归一化之后，模型测试精度可以达到 0.9 左右，批量归一化是一种现在仍然被广泛使用的技术。


批量归一化的细节：
针对卷积的批量归一化，卷积层会抽取出不同的特征，我们把每个像素点看作一个样本，其在不同通道上有不同的值，形成一个向量看作一个样本点，根据这些样本点求均值。