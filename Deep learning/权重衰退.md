**主要内容** 利用pytorch实现权重衰退，权重衰退通过限制参数的取值范围来降低模型复杂度，从而缓解过拟合现象。越大的lambda提供越强的正则。

- 代码部分：

1 - 人造数据集
```
num_inputs, num_train, num_test, batch_size = 100, 20, 100, 5  
true_w = torch.ones(num_inputs, 1) * 0.01  
true_b = 0.05  
train_features, train_labels = synthetic_dataset(true_w, true_b, num_train)  
test_features, test_labels = synthetic_dataset(true_w, true_b, num_test)
```
2 - l2正则项
```
def l2_penalty(w):  
    return torch.sum(w.pow(2)) / 2
```
3 - 训练
```
def train(lamb_da):  
    net = nn.Sequential(nn.Linear(num_inputs, 1))  
    epoch, lr = 100, 0.03  
    loss = nn.MSELoss()  
    trainer = torch.optim.SGD(net.parameters(), lr=lr)  
    for i in range(epoch):  
        for X, y in data_iter(train_features, train_labels, batch_size):  
            loss_value = loss(net(X), y) + lamb_da * l2_penalty(net[0].weight)  
            trainer.zero_grad()  
            loss_value.backward()  
            trainer.step()  
    loss_value = loss(net(train_features), train_labels) + lamb_da * l2_penalty(net[0].weight)  
    print(f'loss_value: {loss_value} (on train_set)')  
    loss_value = loss(net(test_features), test_labels) + lamb_da * l2_penalty(net[0].weight)  
    print(f'loss_value: {loss_value} (on test_set)')

# 不进行正则化，lambda = 0  
train(0)  
# 进行正则化, lambda = 1  
train(1)
```
4 - 运行结果
![[Pasted image 20241004164014.png]]

可以发现，不用正则的时候，模型在训练集上表现很好，但在测试集上效果较差；
而使用正则的时候，模型虽然在训练集上没有那么好，但是在测试集上表现不错，也就是说，加入正则项后训练的模型具有更强的泛化能力。