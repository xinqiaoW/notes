**主要内容** 我实现了样式迁移，样式迁移的训练参数不是网络参数，而是生成的图片。损失包括风格损失，图片平滑度损失和内容损失，内容损失主要看生成图片和内容图片内容层上像素之间的差距，图片平滑度损失主要看生成图片像素上下左右的差值。风格损失主要看风格图片和生成图片在风格层上的统计信息的差值。我们最小化损失即可。

- 代码：

1 - 网络下载
```
net = torchvision.models.vgg19(weights=torchvision.models.VGG19_Weights.DEFAULT)  
style_layers, content_layers = [0, 5, 10, 19, 28], [25]  
rgb_mean = torch.tensor([0.485, 0.456, 0.406])  
rgb_std = torch.tensor([0.229, 0.224, 0.225])
```
2 - 图片预处理和后处理
```
def preprocess(img, image_shape):  
    trans = torchvision.transforms.Compose([  
        torchvision.transforms.Resize(image_shape),  
        torchvision.transforms.ToTensor(),  
        torchvision.transforms.Normalize(rgb_mean, rgb_std)  
    ])  
    return trans(img).unsqueeze(0)  
  
  
def postprocess(img):  
    img = img[0].cpu().detach()  
    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)  
    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))
```
3 - 提取特征层、内容层信息的函数
```
def extract_features(X, content_layers, style_layers):  
    contents = []  
    styles = []  
    for i in range(len(net)):  
        X = net[i](X)  
        if i in style_layers:  
            styles.append(X)  
        if i in content_layers:  
            contents.append(X)  
    return contents, styles
```
4 - 损失函数
```
def content_loss(Y_hat, Y):  
    return torch.square(Y_hat - Y.detach()).mean()  
  
  
def gram(X):  
    num_channels, n = X.shape[1], X.shape[2] * X.shape[3]  
    X = X.reshape((num_channels, n))  
    return torch.matmul(X, X.T) / (n * num_channels)  
  
  
def style_loss(Y_hat, gram_Y):  
    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()  
  
  
def tv_loss(Y_hat):  
    return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +  
                torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())  
  
  
content_weight, style_weight, tv_weight = 1, 1e4, 10  
  
  
def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y):  
    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(contents_Y_hat, contents_Y)]  
    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(styles_Y_hat, styles_Y)]  
    tv_1 = tv_loss(X) * tv_weight  
    return sum(contents_l + styles_l + [tv_1])
```
5 - 定义训练对象
```
class SynthesizedImage(nn.Module):  
    def __init__(self, image_shape, **kwargs):  
        super(SynthesizedImage, self).__init__(**kwargs)  
        self.weight = nn.Parameter(torch.rand(*image_shape))  
  
    def forward(self):  
        return self.weight
```
6 - 训练过程
```
def get_inits(X, device, lr, styles_Y):  
    gen_img = SynthesizedImage(X.shape).to(device)  
    gen_img.weight.data.copy_(X.data)  
    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)  
    styles_Y_gram = [gram(Y) for Y in styles_Y]  
    return gen_img(), styles_Y_gram, trainer  
  
  
def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):  
    X, styles_Y, trainer = get_inits(X, device, lr, styles_Y)  
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)  
    for i in range(num_epochs):  
        trainer.zero_grad()  
        contents_pred, styles_pred = extract_features(X, content_layers, style_layers)  
        l = compute_loss(X, contents_pred, styles_pred, contents_Y, styles_Y)  
        print(X)  
        l.backward()  
        trainer.step()  
        scheduler.step()  
    return X  
  
  
X = preprocess(Image.open('./content.png').convert('RGB'), (400, 500))  
Y = preprocess(Image.open('./style.png').convert('RGB'), (400, 500))  
_, styles_Y = extract_features(Y, [], style_layers)  
content_Y, _ = extract_features(X, content_layers, [])
  
X = train(X, content_Y, styles_Y, 'cuda:0', 0.3, 500, 200)  
X = postprocess(X)  
X.save('output.png')
```
7 - 训练结果
风格图片：
![[style.png]]
内容图片：
![[content.png]]
训练结果：
![[屏幕截图 2024-10-24 133702.png]]

