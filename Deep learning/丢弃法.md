**主要内容** 我实现了丢弃法，丢弃法通过对神经元随机丢弃，实现了更好的泛化性。神经元随机丢弃类似于给训练集添加噪声，相当于一个正则。

- 代码部分：

1 - 导入必要的包和库：
```
import torch.nn as nn  
import torch  
import numpy as np  
import matplotlib.pyplot as plt  
import torchvision  
from torch.utils import data  
from torchvision import transforms  
import gif
```
2 - 加载数据集
```
mnist_train, mnist_test = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor()), torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())  
train_loader = data.DataLoader(mnist_train, batch_size=256, shuffle=True)  
test_loader = data.DataLoader(mnist_test, batch_size=256, shuffle=False)
```
3 - 定义丢弃层
```
def dropout_layer(X, dropout_possibility):  
    assert 0 <= dropout_possibility <= 1  
    if dropout_possibility == 1:  
        return torch.zeros_like(X)  
    mask = torch.rand(X.shape) > dropout_possibility  
    return mask * X / (1.0 - dropout_possibility)
```
4 - 定义flatten函数
```
def flatten(X):  
    x = np.array([i.reshape(28 * 28) for i in X])  
    return torch.tensor(x)
```
5 - 定义神经网络
```
class Net(nn.Module):  
    def __init__(self, num_inputs, num_hidden_1, num_hidden_2, num_outputs, is_no_dropout=True):  
        super(Net, self).__init__()  
        self.lin1 = nn.Linear(num_inputs, num_hidden_1)  
        self.lin2 = nn.Linear(num_hidden_1, num_hidden_2)  
        self.lin3 = nn.Linear(num_hidden_2, num_outputs)  
        self.relu = nn.ReLU()  
        self.is_no_dropout = is_no_dropout  
  
    def forward(self, x):  
        h1 = self.relu(self.lin1(x))  
        if self.is_no_dropout:  
            h1 = dropout_layer(h1, 0.0)  
        h2 = self.relu(self.lin2(h1))  
        if self.is_no_dropout:  
            h2 = dropout_layer(h2, 0.0)  
        out = self.lin3(h2)  
        return out
```
6 - 训练
```
num_inputs, num_hidden_1, num_hidden_2, num_outputs = 784, 256, 256, 10
history_accuracy_train = []  
history_accuracy_test = []  
net = Net(num_inputs, num_hidden_1, num_hidden_2, num_outputs)  
num_epochs, batch_size, lr = 10, 256, 0.5  
loss = nn.CrossEntropyLoss()  
trainer = torch.optim.SGD(net.parameters(), lr)  
cnt = 0  
for i in range(num_epochs):  
    for X, y in train_loader:  
        X = flatten(X)  
        loss_value = loss(net.forward(X), y)  
        trainer.zero_grad()  
        loss_value.backward()  
        trainer.step()  
        cnt += 1  
        if cnt % 100 == 0:  
            with torch.no_grad():  
                net.is_no_dropout = False  
                prediction = torch.argmax(net.forward(flatten(mnist_train.data.float())), dim=1)  
                accuracy_train = (prediction == mnist_train.targets.float()).float().mean()  
                history_accuracy_train.append(accuracy_train)  
                prediction = torch.argmax(net.forward(flatten(mnist_test.data.float())), dim=1)  
                accuracy_test = (prediction == mnist_test.targets.float()).float().mean()  
                history_accuracy_test.append(accuracy_test)  
                net.is_no_dropout = True
```
7 - 绘制动图
```
@gif.frame  
def update_plot(num, epochs, history_train_accuracy, history_test_accuracy):  
    fig, ax = plt.subplots(1, figsize=(10, 10))  
    X= torch.arange(num)  
    y = torch.tensor(history_train_accuracy[:num])  
    ax.plot(X, y, 'r-', label='train accuracy')  
    y = torch.tensor(history_test_accuracy[:num])  
    ax.plot(X, y, 'b-', label='test accuracy')  
    ax.legend()  
    ax.set_xlim(0, epochs)  
    ax.set_yscale('log')  
    ax.set_ylim(1e-1, 1e0)  
    ax.set_xlabel('iter')  
    ax.set_ylabel('accuracy')  
    ax.set_title('accuracy vs iter')  
  
  
frames = []  
for i in range(len(history_accuracy_train)):  
    frame = update_plot(i, len(history_accuracy_train), history_accuracy_train, history_accuracy_test)  
    frames.append(frame)  
gif.save(frames, 'dropout.gif', duration=4)
```
8 - 结果展示
![[dropout_yes.gif]]
这是使用丢弃法的结果。
![[dropout.gif]]
这是未使用丢弃法的结果。仅从这个例子来看，丢弃法并没有取得很好的效果。

丢弃法在验证的时候不需要进行丢弃操作，在pytorch框架下，我们可以通过 $net.eval()$ 关闭丢弃层。

$Hinton$ 的想法是 丢弃掉一些神经元后，相当于利用神经网络的一些子网络进行训练，最后的结果是所有子网络的一个平均结果，因而效果更好。

业界普遍认为，丢弃法是一种正则，一篇paper曾经将丢弃法的结果和其他技术的结果做对比，发现丢弃法的效果类似与一种正则。

仔细想想，似乎只要我们对网络的参数取值范围做出一定的约束，或者制造一定的随机性，那么便可以达到正则的效果。