**主要内容** 利用 pytorch “几乎” 从零实现了线性模型， 了解了pytorch底层计算图、自动求导的原理，学会了利用 $yield$ 制作迭代器，从而便捷地提供遍历迭代器。同时numpy库和torch库的运算不能混用。

底层计算图：
针对 $(requires\_grad=True)$ 这类张量，我们对其进行操作时，例如加减乘除等等，pytorch会自动在内存中保存对应的计算图，所以如果我们对张量操作时不需要偏导，我们可以利用上下文管理器torch.no_grad()，阻止计算图的生成。

- 代码部分：

1 - 导入必要的包和库 
```
import numpy as np  
import matplotlib.pyplot as plt  
import torch.nn as nn  
import torch  
import random
```
2 - 制造人工数据集
```
def synthetic_dataset(w, b, num_examples):  
    x = torch.normal(0, 1, size=(num_examples, len(w)))  
    y = torch.matmul(x, w) + b  
    y += torch.normal(0, 0.01, size=y.shape)  
    return x, y.reshape(-1, 1)  
  
  
true_w = torch.tensor([2, -3.4])  
true_b = 4.2  
features, labels = synthetic_dataset(true_w, true_b, 1000)
```
3 - **数据迭代器**
```
def data_iter(features, labels, batch_size):  
    num_examples = len(features)  
    indices = list(range(num_examples))  
    random.shuffle(indices)  
    for i in range(0, num_examples, batch_size):  
        batch_indices = torch.tensor(indices[i:min(i+batch_size, num_examples)])  
        yield features[batch_indices], labels[batch_indices]
```
4 - 模型参数初始化
```
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)  
b = torch.zeros(1, requires_grad=True)
```
5 - 损失函数、前向传播、参数更新
```
# 定义前向传播（计算结果）  
def linreg(X, w, b):  
    return torch.matmul(X, w) + b  
  
  
# 定义均方损失  
def squared_loss(y_hat, y):  
    return ((y_hat - y.reshape(y_hat.shape)) ** 2 / (2 * len(y))).sum()  
  
  
# 参数更新  
def param_update(params, lr):  
    with torch.no_grad():  
        for param in params:  
            param -= lr * param.grad  
            param.grad.zero_()
```
6 - 训练代码
```
lr = 0.15  
epochs = 3  
batch_size = 10  
  
for i in range(epochs):  
    for X, y in data_iter(features, labels, batch_size):  
        loss_value = squared_loss(linreg(X, w, b), y)  
        loss_value.backward()  
        param_update([w, b], lr)  
    with torch.no_grad():  
        loss_value = squared_loss(linreg(features, w, b), labels)  
        print(f"Epoch {i+1}, Loss: {loss_value.item()}")  
  
print(w - true_w.reshape(w.shape))  
print(b - true_b)
```
