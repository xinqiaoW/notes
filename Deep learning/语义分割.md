**主要内容** 我首先实现了语义分割数据集的构造，随后利用 FCN 实现了一个简单的语义分割程序。语义分割的关键在于输出是很大的，每个像素都要为其分类。所以我们需要转置卷积获得更大的输出。除此之外，我们在数据预处理时，采用哈希映射的方法为每个像素分配编号（因为我们数据集的标号是三维向量（R，G，B），每个RGB对应某一类）。

- 代码部分：

1 - 读取数据集
```
def read_voc_images(is_train=True):  
    data_dir = os.path.join('C:\\', 'Users', 'www', 'PycharmProjects', 'Machine_learning',  
                            'VOC2012')  
    txt_fname = os.path.join(data_dir, 'ImageSets', 'Segmentation', 'train.txt' if is_train else 'val.txt')  
    mode = torchvision.io.image.ImageReadMode.RGB  
    with open(txt_fname, 'r') as f:  
        images = f.read().split()  
    features, labels = [], []  
    for i in images:  
        features.append(torchvision.io.read_image(os.path.join(data_dir, 'JPEGImages', f'{i}.jpg')))  
        labels.append(torchvision.io.read_image(os.path.join(data_dir, 'SegmentationClass', f'{i}.png'), mode=mode))  
    return features, labels
```
2 - 设置标号
```
VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],  
               [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],  
               [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],  
               [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],  
                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],  
               [0, 64, 128]]  
  
  
VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',  
               'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',  
               'dog', 'horse', 'motorbike', 'person', 'potted plant', 'sheep',  
               'sofa', 'train', 'tv/monitor']  
  
  
# 查找标签中每个像素的类索引  
def colormap2label():  
    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)  
    for i, colormap in enumerate(VOC_COLORMAP):  
        colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i  
    return colormap2label  
  
  
def voc_label_indices(colormap, colormap2label):  
    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')
    idx = (colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256 + colormap[:, :, 2]  
	return colormap2label[idx]
```
3 - 图片裁剪
```
def voc_rand_crop(feature, label, height, width):  
    rect = torchvision.transforms.RandomCrop.get_params(feature, (height, width))  
    feature = torchvision.transforms.functional.crop(feature, *rect)  
    label = torchvision.transforms.functional.crop(label, *rect)  
    return feature, label
```
4 - 数据集
```
class VOCSegDataset(torch.utils.data.Dataset):  
    def __init__(self, is_train, crop_size, colormap2label):  
        self.transform = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  
        self.crop_size = crop_size  
        features, labels = read_voc_images(is_train=is_train)  
        self.features = [self.normalize_image(feature) for feature in self.filter(features)]  
        self.labels = self.filter(labels)  
        self.colormap2label = colormap2label  
  
    def normalize_image(self, img):  
        return self.transform(img.float())  
  
    def filter(self, imgs):  
        return [img for img in imgs if (img.shape[1] >= self.crop_size[0] and img.shape[2] >= self.crop_size[1])]  
  
    def __getitem__(self, idx):  
        img, lbl = voc_rand_crop(self.features[idx], self.labels[idx], *self.crop_size)  
        return img, voc_label_indices(lbl, self.colormap2label)
        
    def __len__(self):  
	    return len(self.features)
```
5 - 选择网络并初始化
```
pretrained_net = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)  
net = nn.Sequential(*list(pretrained_net.children())[:-2])  
  
num_classes = 21  
net.add_module('final_conv',  
               nn.Conv2d(512, num_classes, kernel_size=1))  
net.add_module('transpose_conv',  
               nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, padding=16))


def bilinear_kernel(in_channels, out_channels, kernel_size):  
    factor = (kernel_size + 1) // 2  
    if kernel_size % 2 == 1:  
        center = factor - 1  
    else:  
        center = factor - 0.5  
    og = (torch.arange(kernel_size).reshape(-1, 1),  
          torch.arange(kernel_size).reshape(1, -1))  
    filt = (1 - torch.abs(og[0] - center) / factor) * (1 - torch.abs(og[1] - center) / factor)  
    weight = torch.zeros((in_channels, out_channels, kernel_size, kernel_size))  
    weight[range(in_channels), range(out_channels), :, :] = filt  
    return weight


W = bilinear_kernel(num_classes, num_classes, 64)  
net.transpose_conv.weight.data.copy_(W)
```
6 - 损失函数
```
def loss(inputs, targets):  
    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)
```
7 - 模型训练
```
def train_ch13(net, train_iter, test_iter, num_epochs, lr, device):  
    train_acc_history = []  
    test_acc_history = []  
    print('training on', device)  
    net.to(device)  
    trainer = torch.optim.SGD(net.parameters(), lr=lr)  
    for i in range(num_epochs):  
        net.train()  
        for X, y in train_iter:  
            X, y = X.to(device), y.to(device)  
            loss_value = loss(net(X), y)  
            trainer.zero_grad()  
            loss_value.sum().backward()  
            trainer.step()  
        train_acc = evaluate_accuracy_gpu(net, train_iter, device)  
        train_acc_history.append(train_acc)  
        test_acc = evaluate_accuracy_gpu(net, test_iter, device)  
        test_acc_history.append(test_acc)  
    frames = []  
    for i in range(len(train_acc_history)):  
        frame = update_plot([np.arange(i + 1), train_acc_history[:i+1], 'train_acc', 'o', 'b'], [np.arange(i + 1), test_acc_history[:i+1], 'test_acc', 'o', 'r'], x_scale='linear', y_scale='log', y_lim=(1e-1, 1), x_lim=(0, num_epochs), x_label='epoch', y_label='accuracy', title='Accuracy vs epoch', fig_size=(8, 8))  
        frames.append(frame)  
    gif.save(frames, 'semantic_segmentation.gif', duration=500)  
  
  
train_ch13(net, train_iter, test_iter, num_epochs, lr, device)
```
8 - 训练结果
![[semantic_segmentation.gif]]
测试精度在 0.85 左右，对于语义分割这并不是一个很好的精度，因为一张图片的像素很多，大部分是比较容易预测的，只有边缘比较难，所以 0.85 意味着边缘基本没做到预测，也就是效果并不好。