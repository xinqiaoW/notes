**主要内容** 我实现了转置卷积的基本操作，并了解了转置卷积和卷积之间的基本关联。

转置卷积可以将输入矩阵扩大，而且扩大后的矩阵包含着原来矩阵的信息，类似于卷积的逆过程，卷积将大块信息浓缩，而转置卷积试图将浓缩的信息表达在一大块上，想要‘还原’浓缩的信息。

转置卷积等价于先对输入做padding，再将卷积核上下、左右颠倒后，进行卷积操作。

- 代码部分：

1 - 转置卷积的定义及使用范例
```
def trans_conv(X, K):  
    h, w = K.shape  
    Y = torch.zeros((X.shape[0] - 1 + h, X.shape[1] - 1 + w))  
    for i in range(X.shape[0]):  
        for j in range(X.shape[1]):  
            Y[i:i+h, j:j+w] += X[i, j] * K  
    return Y

X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])  
Y = torch.tensor([[0.0, 1.0], [2.0, 3.0]])  
print(trans_conv(X, Y))
```
2 - 调包
```
  
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False, stride=2)  
X, Y = X.reshape(1, 1, 2, 2), Y.reshape(1, 1, 2, 2)  
tconv.weight.data = Y  
print(tconv(X))
```
3 - 与卷积之间的联系
```
input = torch.tensor([[0.0, 1.0], [2.0, 3.0]])  
kernel = torch.tensor([[0.0, 1.0], [2.0, 3.0]])  
kernel = kernel.flip(0).flip(1)  
  
  
def padding_(X, pad):  
    Y = torch.zeros((X.shape[0] + pad * 2, X.shape[1] + pad * 2))  
    Y[pad: pad + X.shape[0], pad: pad + X.shape[1]] = X  
    return Y  
  
  
input = padding_(input, 1).reshape(1, 1, 4, 4)  
conv = nn.Conv2d(1, 1, kernel_size=2, bias=False)  
conv.weight.data = kernel.reshape(1, 1, 2, 2)  
print(conv(input))
```
