Transformer-Captioning：
	在这个作业中，作者将 图片的特征向量 当作 key，对于每个样本 key 的形状为（1， D），
	也就是说 key 的序列长度为 1，注意力机制完全不起作用。我们可以把图片分割成许许多多的patch，使 key 的序列长度不为 1，这样注意力机制就可以注意到 更关键的 patch。