1）sgd + momentum：
综合利用过去的梯度信息，和现在的梯度信息，使得优化过程更加得稳定，可以越过极小值点或鞍点，同时，由于速度带动量，可以抵消一些不平均的条件，使得模型可以在‘taco’型上训练的更快。
2）Adagrad：
Adagrad 加入了一个积累的平方开根项，可以用来记录之前的梯度，如果在某个方向上之前的梯度过大，在除以这一项后，使得在这个方向上的梯度被减弱，使得模型在条件对结果影响不平衡的情况下，也可以很快的训练。
3）RMSprop：
利用移动平均平方开根项，避免了训练的最后，模型的梯度太小，无法进步。
4）Adam：
结合以上方式的所有优点构成的优化器。