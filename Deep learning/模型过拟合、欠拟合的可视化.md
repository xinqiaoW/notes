**主要内容** 进行了模型过拟合、欠拟合的直观尝试，我们可以发现，欠拟合时模型不能很好的解释数据，过拟合时模型错误地拟合了噪声。

- 代码部分：

1 - 搭建人工数据集·

```
# 搭建人工数据集y = 1 + 2x + 3x^2 + 4x^3  
train_num = 200  
test_num = 60  
features = np.random.normal(size=(train_num + test_num, 1))  
np.random.shuffle(features)  
features = np.power(features, np.arange(20).reshape(1, -1))  
true_w = np.zeros(20)  
true_w[:4] = np.array([1, 2, 3, 4])  
labels = np.dot(features, true_w)  
labels += np.random.normal(scale=0.1, size=labels.shape)  
features, labels = [torch.tensor(x, dtype=torch.float32)for x in [features, labels]] 
```
2 - 记录损失的函数
```
  
  
def data_iter(features, labels, batch_size):  
    num_examples = len(features)  
    indices = list(range(num_examples))  
    random.shuffle(indices)  
    for i in range(0, num_examples, batch_size):  
        batch_indices = torch.tensor(indices[i:min(i+batch_size, num_examples)])  
        yield features[batch_indices], labels[batch_indices]  
  
  
# 函数：给定不同的特征，用同一个模型进行训练，得到损失  
def train(train_features, train_labels, test_features, test_labels):  
    net = nn.Sequential(nn.Linear(train_features.shape[1], 1, bias=False))  
    epoch = 400  
    history_loss = []  
    loss = nn.MSELoss()  
    trainer = torch.optim.SGD(net.parameters(), lr=0.001)  
    for i in range(epoch):  
        for X, y in data_iter(train_features, train_labels, 10):  
            trainer.zero_grad()  
            prediction = net(X)  
            loss_value = loss(prediction.reshape(y.shape), y)  
            loss_value.backward()  
            trainer.step()  
        if i % 10 == 0 or i == 0:  
            history_loss.append(loss_value.item())  
    test_loss = loss(net(test_features).reshape(test_labels.shape), test_labels)  
    return test_loss, history_loss  
```
3 - 进行可视化，绘制散点图
```
def show_scatter(x, y, x_label, y_label, title):  
    plt.scatter(x, y, color='b', marker='o')  
    plt.xlabel(x_label)  
    plt.ylabel(y_label)  
    plt.title(title)  
    plt.show()  
  
  
# 进行可视化  
train_feature = features[:train_num, :2]  
test_loss_feature_2, history_loss_feature_2 = train(train_feature, labels[:train_num], features[train_num:, :2], labels[train_num:])  
show_scatter(np.arange(40), history_loss_feature_2, 'epoch', 'loss', 'loss vs epoch')  
train_feature = features[:train_num, :4]  
test_loss_feature_4, history_loss_feature_4 = train(train_feature, labels[:train_num], features[train_num:, :4], labels[train_num:])  
show_scatter(np.arange(40), history_loss_feature_4, 'epoch', 'loss', 'loss vs epoch')  
test_loss, history_loss = train(features[:train_num, :], labels[:train_num], features[train_num:, :], labels[train_num:])  
show_scatter(np.arange(40), history_loss, 'epoch', 'loss', 'loss vs epoch')
```

- 结果(蓝色的点是模型在训练集下 损失 $loss$ 和 $epoch$ 之间的关系，红色是模型在测试集上的损失)：
![[Figure_1 1.png]]
只选择两个特征训练时，模型在训练集和测试集中表现都不是很好，模型不能很好地提取训练集中的信息。

![[Figure_2 1.png]]
选择四个特征训练时，模型在训练集和测试集中的效果都很好。

选择所有特征训练时，模型错误的学习了训练集中的噪音，导致在训练集中效果很好，但测试集中效果很差。这里我犯了一个错误，我需要拟合的函数时 $y=1+2x+3x^2+4x^3$ ，我们的 $features$ 共有 $20$ 列，选择所有的特征训练时，实际上我做的是 找到 $w_0,w_1,w_2,w_3,……,w_{19}$ 使得函数 $y=w_0x^0+w_1x^1+……+w_{19}x^{19}$ 能很好地拟合训练数据，但是 $x^{19}$ 可能会产生一个超大的数，$loss$ 很容易 $nan$ ，同时更新参数时，即使 $w_{19}$ 只更新了一点点，预测结果也可能巨震，因此我重新设定了 $features$.