边缘框的表示方法：
- （左上$x$, 左上$y$，右上$x$, 右上$y$）
- （左上$x$, 左上$y$，宽width，高height）
四个值可以确定一个边缘框

标记数据集：利用一个向量，【图片，标号（种类），边缘框】

锚框：提出多个被称为锚框的区域，将锚框与背景或者某一类关联。

J指数（IoU）：用来形容给定两个集合的相似程度
$$J(A, B) = \frac{{\left|A\cap B\right|}}{{\left|A\cup B\right|}}$$

我们可以把锚框内的像素点看做一个集合，${\left|A\cap B\right|}$表示两个锚框内公共像素点组成的集合的基数，${\left|A\cup B\right|}$表示两个锚框内所有像素点组成的集合的基数。

直觉认识这个公式：
1.当${\left|A\cup B\right|}$=${\left|A\cap B\right|}$时，明显$J(A, B)$=1，A与B完全相同（J值越接近1，说明A与B相似程度越大）；
2.当${\left|A\cap B\right|}$<${\left|A\cup B\right|}$时，$J(A, B)$<1；
3.当${\left|A\cap B\right|}$=0时，A和B没有任何的公共元素，$J(A, B)$=0；
4.假设存在两对$(A, B)$（均满足${\left|A\cap B\right|}$=0），其中$A_1,B_1$均只含一个元素，$A_2,B_2$均含有大量元素，那么根据$J$指数，$J_1,J_2$均为0，$A_1,B_1$只有一个元素不相同，所以直觉上我们会认为，$A_2,B_2$的差异更大，$J$指数并不能量化这一点。


## 模拟
$$A=\left[
\begin{matrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\
x_{31} & x_{32} & x_{33} \\
x_{41} & x_{42} & x_{43} \\
x_{51} & x_{52} & x_{53} 
\end{matrix}
\right]
$$

$A$ 是一个$5*3$的矩阵，我们生成了五个锚框，原图片有三个边缘框，$x_{ij}$ 表示锚框 $i$ 和边缘框 $j$ 的$IoU$ 值，我们假设$A$中最大元素为$x_{22}$ , 即锚框 $2$ 和边缘框 $2$ 的相似程度最高，我们关联锚框 $2$ 和边缘框 $2$ ，随后我们删除第 $2$ 行和第 $2$ 列，在剩下的矩阵中寻找最大值，以此类推，值得注意的是，**我们可能会进行多轮，也就是说，关联某个边缘框的锚框可能不止一个** 

## 非极大值抑制

由于关联某个边缘框的锚框可能不止一个，如果我们把所有锚框都显示在画面中的话，显得很凌乱，而且并不是很有意义，所以我们要抑制某些输出。

- 选中非背景框的最大预测值
- 删除所有和它 $IoU$ 值大于阈值 $θ$ 的预测 -> 删除相似预测
- 重复上述操作，直到所有锚框要么被选中，要么被删除


## 代码

1 - 香蕉检测数据集
```
def load_bananas_dataset(is_train=True):  
    csv_frame = os.path.join('C:\\', 'Users', 'www', 'PycharmProjects', 'Machine_learning', 'banana_detection',  
                             'bananas_train' if is_train else 'bananas_val', 'label.csv')  
    csv_data = pd.read_csv(csv_frame)  
    csv_data.set_index('img_name')  
    images, targets = [], []  
    for img_name, target in csv_data.iterrows():  
        images.append(torchvision.io.read_image(os.path.join('C:\\', 'Users', 'www', 'PycharmProjects',  
                                                             'Machine_learning',  
                                                             'banana_detection',  
                                                             'bananas_train' if is_train else 'bananas_val',  
                                                             'images', f'{img_name}.png')))  
        targets.append(list(target)[1:])  
    return images, torch.tensor(targets).unsqueeze(1) / 256
  
  
# 定义数据集类  
class BananasDataset(torch.utils.data.Dataset):  
    def __init__(self, is_train=True):  
        self.features, self.labels = load_bananas_dataset(is_train)  
  
    def __len__(self):  
        return len(self.features)  
  
    def __getitem__(self, idx):  
        return self.features[idx].float(), self.labels[idx]
```

标签 labels 一共有 3 个维度，第一个维度表示批量， 第二个维度表示每张图片最多有几个物体，第三个维度蕴含类别和边缘框信息。
2 - 锚框生成函数
```
# 定义锚框生成函数  
def multi_boxes(data, sizes, ratios):  
    num_sizes, num_ratios = len(sizes), len(ratios)  
    num_boxes = num_sizes + num_ratios - 1  
    in_height, in_width = data.shape[-2:]  
    sizes, ratios = torch.tensor(sizes), torch.tensor(ratios)  
  
    # 生成归一化像素网格  
    shift_x, shift_y = torch.meshgrid((torch.arange(in_width) + 0.5) / in_width,  
                                      (torch.arange(in_height) + 0.5) / in_height, indexing='xy')  
    shift_x = shift_x.reshape(-1)  
    shift_y = shift_y.reshape(-1)  
    out_grids = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1).repeat_interleave(num_boxes, dim=0)  
  
    # 生成锚框的大小和宽度  
    w = torch.cat((sizes * torch.sqrt(ratios[0]),  
                  sizes[0] * torch.sqrt(ratios[1:]))) * in_height  
    h = torch.cat((sizes / torch.sqrt(ratios[0]),  
                  sizes[0] / torch.sqrt(ratios[1:]))) * in_height  
  
    boxes_all = torch.stack((-w, -h, w, h), dim=1).repeat(in_height * in_width, 1) / 2  
  
    return (out_grids + boxes_all).unsqueeze(0)
```
torch.cat() 可以将两个向量拼接在一起，例如
tensor_1 = torch.tensor(\[1, 2, 3])
tensor_2 = torch.tensor(\[3, 6, 9])
torch.cat(tensor_1, tensor_2)为torch.tensor(\[1, 2, 3, 3, 6, 9])
torch.stack((w1,w2), dim=k): 则重组后的张量满足为A相对于w1，w2增加了一个维度k，维度k上有两个元素，分别拼接着w1和w2.详情可参考：[pytorch中stack方法的总结和理解](https://www.cnblogs.com/tangzj/p/15526544.html)
torch.repeat_interleave():举个例子 将\[\[1, 2, 3], \[4, 5, 6]] 重复成\[\[1, 2, 3], \[1, 2, 3], \[4, 5, 6], \[4, 5, 6]]
本函数先生成像素网格网络，再生成偏移张量，通过对每个像素点做偏移，得到边缘框。这种方式生成的边缘框数量相当多（由于每个像素都要生成若干边缘框）
3 - 计算IOU的函数
```
# 计算IoU值的函数  
def box_iou(boxes1,boxes2):  
    """计算两个锚框或边界框列表中成对的交并比"""  
    # 定义一个lambda函数，计算一个锚框或边界框的面积  
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))  
    areas1 = box_area(boxes1)  
    areas2 = box_area(boxes2)  
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])  
    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)  
    inter_areas = inters[:, :, 0] * inters[:, :, 1]  
    union_areas = areas1[:, None] + areas2 - inter_areas  
    return inter_areas / union_areas
```
tensor\[: , 1] 是一个一维向量，tensor\[: , None, 1] 通过None增添了一个新的维度，维数为2
torch.max(tensor_1, tensor_2)可以比较两个张量的最大值，也可以形成最大值矩阵，形状相同直接对应元素比较，形状不同时，会形成最大值矩阵。
 - （2，2，3）与（2，3）
```
x = torch.tensor([[[1, 2, 3],  
                   [4, 5, 6]],  
                  [[2, 4, 6],  
                   [8, 7, 6]]])  
y = torch.tensor([[2, 1, 3],  
                  [5, 4, 3]])
```
![[Pasted image 20241016123555.png]]
- （2，1，3）与（2，3）
 先将（2，1，3）广播为（2，2，3）
```
x = torch.tensor([[[1, 2, 3]],  
                  [[2, 4, 6]]])  
y = torch.tensor([[2, 1, 3],  
                  [5, 4, 3]])  
  
print(torch.max(x, y))
```
![[Pasted image 20241016123815.png]]

4 - 定义绘制锚框函数
```
fig, axes = plt.subplots(figsize=(10, 6))  
  
  
def show_bboxes(images, bboxes, axes_in):  
    axes_in.imshow(images)
    for i in bboxes：
	    rect = Rectangle((i[0], i[1]), i[2] - i[0], i[3] - i[1], color='red', fill=False)  
	    axes_in.add_patch(rect)
	plt.show()
```
5 - 将真实边界框分配给锚框
```
def assign_anchor(ground_truth, anchors, device, threshold):  
    # 将锚框最接近且iou大于阈值的真实边界框分配给锚框  
    m_num_anchors, n_num_ground_truth = anchors.shape[0], ground_truth.shape[0]  
    jaccard = box_iou(anchors, ground_truth)  
    max_ious, indices = torch.max(jaccard, dim=1)  
    mask = torch.nonzero(max_ious >= threshold).reshape(-1)  
    bboxes_map = torch.full((m_num_anchors,), -1, dtype=torch.long, device=device)  
    bboxes_map[mask] = indices[mask]  
    for _ in range(n_num_ground_truth):  
        idx = torch.argmax(jaccard)  
        ground_idx, anchor_idx = (idx % n_num_ground_truth).long(), (idx / n_num_ground_truth).long()  
        bboxes_map[anchor_idx] = ground_idx  
        jaccard[:, ground_idx] = torch.full((m_num_anchors,), -1)  
        jaccard[anchor_idx, :] = torch.full((n_num_ground_truth,), -1)  
    return bboxes_map
```
6 - 定义偏移函数
```
def convert_to_center_form(boxes):  
    return torch.cat(((boxes[:, 2:] + boxes[:, :2]) / 2, boxes[:, 2:] - boxes[:, :2]), axis=1)  
  
  
# 定义偏移函数  
def offset_boxes(anchors, assigned_bb, eps=1e-6):  
    c_anc = convert_to_center_form(anchors)  
    c_assigned_bb = convert_to_center_form(assigned_bb)  
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]  
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])  
    return torch.cat((offset_xy, offset_wh), axis=1)
```
7 - 标记锚框的类和偏移量
```
def multi_box_target(anchors, labels):  
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)  
    batch_class_labels, batch_bbox_mask, batch_offset = [], [], []  
    for i in range(batch_size):  
        label = labels[i, :, :].float()  
        bboxes_map = assign_anchor(label[:, 1:], anchors, 'cpu', 0.5)  
        class_labels = torch.zeros(anchors.shape[0], dtype=torch.long, device=anchors.device)  
        indices_is_assigned_anchor = torch.nonzero(bboxes_map >= 0)  
        indices_assigned_true = bboxes_map[indices_is_assigned_anchor]  
        class_labels[indices_is_assigned_anchor] = label[indices_assigned_true, 0].long() + 1  
        batch_class_labels.append(class_labels)  
        # 掩码  
        bbox_mask = torch.zeros(anchors.shape[0], dtype=torch.float, device=anchors.device)  
        bbox_mask[indices_is_assigned_anchor] = 1  
        bbox_mask = bbox_mask.unsqueeze(-1).repeat(1, 4)  
        batch_bbox_mask.append(bbox_mask.reshape(-1))  
        # 偏移  
        assigned_true_boxes = torch.zeros(anchors.shape[0], 4, device=anchors.device, dtype=torch.float32)  
        assigned_true_boxes[indices_is_assigned_anchor] = label[indices_assigned_true, 1:]  
        offset_ = offset_boxes(anchors, assigned_true_boxes)  
        offset_ *= bbox_mask  
        batch_offset.append(offset_.reshape(-1))  
    offset_bbox = torch.stack(batch_offset)  
    mask_bbox = torch.stack(batch_bbox_mask)  
    class_labels = torch.stack(batch_class_labels)  
    return offset_bbox, mask_bbox, class_labels
```
8 - 偏移的逆过程
```
def offset_inverse(anchors, offset_preds):  
    anc = convert_to_center_form(anchors)  
    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]  
    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]  
    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)  
    predicted_bbox = convert_to_center_form(pred_bbox)  
    return predicted_bbox
```
9 - 非极大值抑制
```
def nms(boxes, scores, iou_threshold):  
    B = torch.argsort(scores, dim=-1, descending=True)  
    keep = []  
    while B.numel() > 0:  
        i = B[0]  
        keep.append(i)  
        if B.numel() == 1:  
            break  
        iou = box_iou(boxes[i].unsqueeze(0),  
                      boxes[B[1:], :]).reshape(-1)  
        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)  
        B = B[inds + 1]  
    return torch.tensor(keep, dtype=torch.long, device=boxes.device)
```
10 - 将非极大值抑制应用于预测
```
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,  
                       pos_threshold=0.009999999):  
    """使用非极大值抑制来预测边界框"""  
    device, batch_size = cls_probs.device, cls_probs.shape[0]    
    anchors = anchors.squeeze(0) 
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]  
    out = []  
    for i in range(batch_size):  
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)  
        conf, class_id = torch.max(cls_prob[1:], 0)   
        predicted_bb = offset_inverse(anchors, offset_pred)  
        keep = nms(predicted_bb, conf, nms_threshold)  
        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)  
        combined = torch.cat((keep, all_idx))  
        uniques, counts = combined.unique(return_counts=True)  
        non_keep = uniques[counts==1]  
        all_id_sorted = torch.cat((keep, non_keep))  
        class_id[non_keep] = -1  
        class_id = class_id[all_id_sorted]  
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]  
        below_min_idx = (conf < pos_threshold)  
        class_id[below_min_idx] = -1  
        conf[below_min_idx] = 1 - conf[below_min_idx]  
        pred_info = torch.cat((class_id.unsqueeze(1),conf.unsqueeze(1),predicted_bb),dim=1)    
        out.append(pred_info)  
    return torch.stack(out)
```
11 - 构建神经网络
```
def cls_predictor(num_inputs, num_anchors, num_classes):  
    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),  
                     kernel_size=3, padding=1)  
  
  
def bbox_predictor(num_inputs, num_anchors):  
    return nn.Conv2d(num_inputs, num_anchors * 4,  
                     kernel_size=3, padding=1)  
  
  
def forward(x, block):  
    return block(x)  
  
  
Y1 = forward(torch.zeros(1, 8, 20, 20), cls_predictor(8, 5, 10))  
print(Y1.shape)  
  
  
def flatten_pred(pred):  
    return torch.flatten(torch.permute(pred, (0, 2, 3, 1)), start_dim=1)  
  
  
def concat_preds(preds):  
    return torch.cat([flatten_pred(p) for p in preds], dim=1)  
  
  
def down_sample_blk(in_channels, out_channels):  
    blk = []  
    for _ in range(2):  
        blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))  
        blk.append(nn.BatchNorm2d(out_channels))  
        blk.append(nn.ReLU())  
        in_channels = out_channels  
    blk.append(nn.MaxPool2d(2))  
    return nn.Sequential(*blk)  
  
  
print(forward(torch.zeros(1, 3, 20, 20), down_sample_blk(3, 10)).shape)  
  
  
def base_net():  
    blk = []  
    num_filter = [3, 16, 32, 64]  
    for i in range(len(num_filter) - 1):  
        blk.append(down_sample_blk(num_filter[i], num_filter[i + 1]))  
    return nn.Sequential(*blk)  
  
  
print(forward(torch.zeros(1, 3, 256, 256), base_net()).shape)  
  
  
def get_blk(i):  
    if i == 0:  
        blk = base_net()  
    elif i == 1:  
        blk = down_sample_blk(64, 128)  
    elif i == 4:  
        blk = nn.AdaptiveMaxPool2d((1, 1))  
    else:  
        blk = down_sample_blk(128, 128)  
    return blk  
  
  
def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):  
    Y = blk(X)  
    anchors = multi_boxes(Y, size, ratio)  
    cls_pred = cls_predictor(Y)  
    bbox_pred = bbox_predictor(Y)  
    return (Y, anchors, cls_pred, bbox_pred)  
  
  
sizes = [[0.2, 0.272],  
         [0.37, 0.447],  
         [0.54, 0.619],  
         [0.71, 0.79],  
         [0.88, 0.961]]  
ratios = [[1, 2, 0.5]] * 5  
num_anchors = len(sizes[0]) + len(ratios[0]) - 1  
  
  
class TinySSD(nn.Module):  
    def __init__(self, num_classes, **kwargs):  
        super(TinySSD, self).__init__(**kwargs)  
        self.num_classes = num_classes  
        idx_in_channels = [64, 128, 128, 128, 128]  
        for i in range(5):  
            setattr(self, f'blk_{i}', get_blk(i))  
            setattr(self, f'cls_{i}', cls_predictor(idx_in_channels[i], num_anchors, num_classes))  
            setattr(self, f'bbox_{i}', bbox_predictor(idx_in_channels[i], num_anchors))  
  
    def forward(self, X):  
        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5  
        for i in range(5):  
            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(X, getattr(self,  
                        f'blk_{i}'), sizes[i], ratios[i], getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))  
  
        anchors = torch.cat(anchors, dim=1)  
        cls_preds = concat_preds(cls_preds)  
        cls_preds = cls_preds.reshape(cls_preds.shape[0], -1, self.num_classes + 1)  
        bbox_preds = concat_preds(bbox_preds)  
        return anchors, cls_preds, bbox_preds  
  
  
# X = torch.zeros((32, 3, 256, 256))  
# model = TinySSD(num_classes=1)  
# anchors, cls_preds, bbox_preds = model(X)  
# print('output anchors:', anchors.shape)  
# print('output class preds:', cls_preds.shape)  
# print('output bbox preds:', bbox_preds.shape)  
  
  
device, batch_size = "cpu", 32  
net = TinySSD(num_classes=1)
```
12 - 损失函数
```
cls_loss = nn.CrossEntropyLoss(reduction='none')  
bbox_loss = nn.L1Loss(reduction='none')  
  
  
def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_mask):  
    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]  
    cls = cls_loss(cls_preds.reshape(-1, num_classes), cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)  
    bbox = bbox_loss(bbox_preds * bbox_mask, bbox_labels * bbox_mask).mean(dim=1)  
    return cls + bbox  
  
  
def cls_eval(cls_preds, cls_labels):  
    return float((cls_preds.argmax(dim=-1).type(  
        cls_labels.dtype) == cls_labels).sum()) / cls_labels.shape[0]  
  
  
def bbox_eval(bbox_preds, bbox_labels, bbox_masks):  
    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())
```
13 - 训练函数
```
def train_obj(net, train_iter, num_epochs, lr, device):  
    cls_loss_history = []  
    bbox_loss_history = []  
    print('training on', device)  
    net.to(device)  
    trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=5e-4)  
    for _ in range(num_epochs):  
        net.train()  
        for X, y in train_iter:  
            trainer.zero_grad()  
            X, y = X.to(device), y.to(device)  
            anchors, cls_preds, bbox_preds = net(X)  
            bbox_labels, bbox_masks, cls_labels = multi_box_target(anchors, y)  
            loss_value = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks)  
            loss_value.mean().backward()  
            trainer.step()  
            net.eval()  
            loss_value = cls_eval(cls_preds, cls_labels)  
            cls_loss_history.append(loss_value)  
            loss_value = bbox_eval(bbox_preds, bbox_labels, bbox_masks)  
            bbox_loss_history.append(loss_value)  
            net.train()  
    frames = []  
    for i in range(len(cls_loss_history)):  
        frame = update_plot([np.arange(i + 1), cls_loss_history[:i+1], 'cls_loss', 'o', 'b'], [np.arange(i + 1), bbox_loss_history[:i+1], 'bbox_loss', 'o', 'r'], x_scale='linear', y_scale='log', y_lim=(1e-4, 1e-2), x_lim=(0, len(cls_loss_history)), x_label='iter', y_label='loss', title='loss vs iter', fig_size=(8, 8))  
        frames.append(frame)  
    gif.save(frames, 'objection_detection.gif', duration=50)  
    net.eval()  
    X = torchvision.io.read_image('../banana_detection/bananas_val/images/0.png').unsqueeze(0)  
    X = X.to(device)  
    anchors, cls_preds, bbox_preds = net(X)  
    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)  
    output = multibox_detection(cls_probs, bbox_preds, anchors)  
    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]  
    out = output[0, idx]  
    axes.imshow(X[0])  
    for i in out:  
        score = i[1]  
        if score >= 0.9:  
            bbox = i[2:]  
            show_bboxes(axes, bbox, f'{score:.2f}', 'w')  
    axes.save('objection_detection.png')  
  
  
train_obj(net, train_iter, 20, 0.2, "cpu")
```
14 - 训练结果
基本可以实现香蕉的检测
15 - 总结

我们构建了一个 5-stage 的卷积神经网络，用来抽取不同尺度上的特征，生成不同尺度的锚框，我们根据真实的 labels 来为每个锚框标记类别，并生成距离真实框的偏移。随后将图片通过网络，预测类别和偏移，通过计算类别损失和偏移损失的加权损失，反向传播，更新网络参数，增强网络对锚框类别以及偏移的预测。

训练好神经网络后，我们将需要目标检测的图片传入网络，得到对锚框分类和偏移的预测，通过非极大值抑制，将一部分锚框删去，再根据 threshold 删去一部分置信度过低的锚框。
最后将锚框（根据偏移预测值移动到可能的真实位置）以及其类别展示出来。
这样就完成了目标检测。

16 - **反思**
==我将锚框的顺序从 由上至下 改进到 由左至右后，效果显著提升，目前不知道具体原因，我推测可能是因为卷积是从左至右扫描的，所以导致卷积更容易提取从左到右的特征。==
