- 代码：
```
import collections  
import re  
import torch  
import torch.nn as nn  
import random  
  
# 一行行阅读文本
def read_time_machine():  
    with open('./time_machine.txt', 'r') as f:  
        lines = f.readlines()  
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]  
  
  
lines = read_time_machine()  
  
# 将lines转化为token
def tokenize(lines, token='word'):  
    if token == 'word':  
        return [line.split(' ') for line in lines]  
    elif token == 'char':  
        return [list(line) for line in lines]  
    else:  
        print('错误，未知令牌类型： ' + token)  
  
# 记录每个token出现的频率
def count_corpus(tokens):  
    if len(tokens) == 0 or isinstance(tokens[0], list):  
        tokens = [token for line in tokens for token in line]  
    return collections.Counter(tokens)  
  
# 创建词汇表类，提供tokens到具体类别标号的字典，以及类别标号到tokens的列表
class Vocab:  
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):  
        if tokens is None:  
            tokens = []  
        if reserved_tokens is None:  
            reserved_tokens = []  
        count = count_corpus(tokens)  
        self.token_freqs = sorted(count.items(), key=lambda x: x[1], reverse=True)  
        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens  
        uniq_tokens += [token for token, freq in self.token_freqs  
                        if freq >= min_freq and token not in uniq_tokens]  
        self.idx_to_token, self.token_to_idx = [], dict()  
        for token in uniq_tokens:  
            self.idx_to_token.append(token)  
            self.token_to_idx[token] = len(self.idx_to_token) - 1  
  
    def __len__(self):  
        return len(self.idx_to_token)  
  
    def __getitem__(self, tokens):  
        if not isinstance(tokens, (list, tuple)):  
            return self.token_to_idx.get(tokens, self.unk)  
        return [self.__getitem__(token) for token in tokens]  
  
    def to_tokens(self, indices):  
        if not isinstance(indices, (list, tuple)):  
            return self.idx_to_token[indices]  
        return [self.idx_to_token[index] for index in indices]  
  
  
tokens = tokenize(lines)  
vocab = Vocab(tokens)  
  
  
def load_corpus_time_machine(max_tokens=-1):  
    lines = read_time_machine()  
    tokens = tokenize(lines, 'word')  
    vocab = Vocab(tokens)  
    corpus = [vocab[token] for line in tokens for token in line]  
    if max_tokens > 0:  
        corpus = corpus[:max_tokens]  
    return corpus, vocab  
  
  
# 语言模型和数据集, 二元语法  
bino_gram_tokens = [pair for pair in zip(tokens[:-1], tokens[1:])]  
bino_gram_vocab = Vocab(bino_gram_tokens)  
  
# 创建批量迭代器
def seq_data_iter_random(corpus, batch_size, num_steps):  
    corpus = corpus[random.randint(0, num_steps - 1):]  
    num_subseqs = (len(corpus) - 1) // num_steps  
    indices = list(range(0, num_subseqs * num_steps, num_steps))  
    random.shuffle(indices)  
  
    def data(pos):  
        return corpus[pos:pos + num_steps]  
  
    num_batches = num_subseqs // batch_size  
    for i in range(0, batch_size * num_batches, batch_size):  
        indices_per_batch = indices[i:i + batch_size]  
        X = [data(j) for j in indices_per_batch]  
        Y = [data(j + 1) for j in indices_per_batch]  
        yield torch.tensor(X), torch.tensor(Y)  
  
# 创建批量迭代器  
def seq_data_iter_sequential(corpus, batch_size, num_steps):  
    offset = random.randint(0, num_steps)  
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size  
    Xs = torch.tensor(corpus[offset:offset + num_tokens])  
    Ys = torch.tensor(corpus[offset + 1:offset + 1 + num_tokens])  
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)  
    num_batches = Xs.shape[1] // num_steps  
    for i in range(0, num_batches * num_steps, num_steps):  
        X = Xs[:, i:i + num_steps]  
        Y = Ys[:, i:i + num_steps]  
        yield X, Y  
  
# 数据加载器类
class SeqDataLoader:  
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):  
        if use_random_iter:  
            self.data_iter_func = seq_data_iter_random  
        else:  
            self.data_iter_func = seq_data_iter_sequential  
        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)  
        self.batch_size, self.num_steps = batch_size, num_steps  
  
    def __iter__(self):  
        return self.data_iter_func(self.corpus, self.batch_size, self.num_steps)  
  
  
def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):  
    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)  
    return data_iter, data_iter.vocab
```
