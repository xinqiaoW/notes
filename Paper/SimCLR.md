对比学习：我们希望模型可以不仅仅从人类标记好的数据中进行学习，也可以从互联网上广大的未标记的数据中学习。

SimCLR 将 同一张图片的两个增强视为一对正样本， 他们在经过神经网络后得到的特征向量应该比较相似，而其他图片均视为负样本，他们经过神经网络后得到的特征向量应当比较不同。

我们可以用$$exp(sim(z_i, z_j) / \tau)$$
来衡量 $z_i$ $z_j$ 的相似程度，其中sim（$z_i$ , $z_j$)是归一化的内积操作，我们都知道，越相似的向量内积也就越大。

训练过程，我们构造好两部分图片（N， D），其中对应行的向量是一对正样本的特征向量，我们的损失函数为

$$Loss = -log(\sum\frac{一对正样本的相似性}{该样本除了自己之外，和其他样本的相似性之和})$$
我们可以发现，Loss越大，说明正样本之间越不相似，越不好。

对比学习的优势在于不同自己标数据，我们可以进行更大规模的网络训练。